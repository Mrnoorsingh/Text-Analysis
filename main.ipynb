{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import parse\n",
    "from nltk.corpus import cmudict,stopwords\n",
    "from utilities import clean_text,report_constraints,score,category,syllable_count,readability,personal_pronoun,PassiveWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of stopwords(Generic)\n",
    "stopword_gen=pd.read_csv(\"StopWords_Generic.txt\")\n",
    "stopword_gen=list(stopword_gen.ABOUT)\n",
    "#create positive negative dictionary from master dictionary\n",
    "df=pd.read_csv(\"LoughranMcDonald_MasterDictionary_2018.csv\")\n",
    "df=df.loc[:,[\"Word\",\"Negative\",\"Positive\"]]\n",
    "df=df[(df.Negative!=0) | (df.Positive!=0)]\n",
    "df=df[~df.Word.isin(stopword_gen)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read contraint and uncertainty files\n",
    "const_file=pd.read_excel(\"constraining_dictionary.xlsx\",sheet_name=0)\n",
    "uncert_file=pd.read_excel(\"uncertainty_dictionary.xlsx\",sheet_name=0)\n",
    "constraints=[word.lower() for word in list(const_file.Word)]\n",
    "uncertainty=[word.lower() for word in list(uncert_file.Word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_url=\"https://www.sec.gov/Archives/\"\n",
    "sections=[\" MANAGEMENTS DISCUSSION AND ANALYSIS\", \" QUANTITATIVE AND QUALITATIVE DISCLOSUR(?:ES|E) ABOUT MARKET RISK\", \" RISK FACTORS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_excel(\"cik_list.xlsx\",sheet_name=0)\n",
    "exl=list(df1[\"SECFNAME\"])\n",
    "row=[]\n",
    "count=0\n",
    "for link in exl:\n",
    "    count+=1\n",
    "    temp=[]\n",
    "    rel_url=link\n",
    "    url=parse.urljoin(abs_url,rel_url)\n",
    "    text=requests.get(url)\n",
    "    \n",
    "    #parse html using beautiful soup\n",
    "    soup=BeautifulSoup(text.content,\"lxml\")\n",
    "    text=soup.get_text()    \n",
    "    cleaned_text=clean_text(text)\n",
    "    \n",
    "    #number of constraints in the report\n",
    "    cleaned_text_=cleaned_text.lower()\n",
    "    total_const=report_constraints(cleaned_text_,constraints)\n",
    "    \n",
    "    for section in sections:\n",
    "        #print(section)\n",
    "        pattern=r\"\\. (?:ITEM|Item) \\.\"+section+r\"(.+?)(?:ITEM|Item) \\.\"\n",
    "        match=re.compile(pattern,flags=re.DOTALL)\n",
    "        substring=match.search(cleaned_text)\n",
    "        try:\n",
    "            substring=substring.group(1)\n",
    "            split_words=substring.split()\n",
    "            #remove periods from the list\n",
    "            clean_list=list(map(lambda x:re.sub(r\"[^a-zA-Z]\",\"\",x),split_words))\n",
    "            clean_list=list(filter(None,clean_list))\n",
    "            #polarity \n",
    "            pos,neg,pol_score,sub_score=score(clean_list,df,stopword_gen)\n",
    "            #category\n",
    "            categ=category(pol_score)\n",
    "            #syllable count\n",
    "            d=cmudict.dict()\n",
    "            syllables=list(map(lambda x:syllable_count(x,d),clean_list))\n",
    "            #average syllable count\n",
    "            avg_syllable=sum(syllables)/len(clean_list)\n",
    "            #complex word count\n",
    "            complex_words=sum(i>2 for i in syllables)\n",
    "            #fog index and average sentence length\n",
    "            fog_id,sent_len,frac=readability(substring,clean_list,complex_words)\n",
    "            #word count\n",
    "            #nltk stopwords\n",
    "            stopword=set(stopwords.words(\"english\"))\n",
    "            word_count=sum(word not in stopword for word in clean_list)\n",
    "            #personal pronouns\n",
    "            p_pronoun=personal_pronoun(substring)\n",
    "            #passive words\n",
    "            pass_word=PassiveWords(substring)\n",
    "            #average word length\n",
    "            avg_word_length=sum(list(map(lambda x:len(x),clean_list)))/len(clean_list)\n",
    "            #constraint and uncertainty scores\n",
    "            const_score=sum(word in constraints for word in clean_list)\n",
    "            uncert_score=sum(word in uncertainty for word in clean_list)\n",
    "            #proportions\n",
    "            pos_prop=pos/word_count\n",
    "            neg_prop=neg/word_count\n",
    "            const_prop=const_score/word_count\n",
    "            uncert_prop=uncert_score/word_count\n",
    "            temp.extend([pos,neg,pol_score,sent_len,frac,fog_id,complex_words,word_count,uncert_score,const_score,pos_prop,neg_prop,uncert_prop,const_prop])\n",
    "            temp=[round(num,2) for num in temp]\n",
    "        except (AttributeError,ZeroDivisionError):\n",
    "            temp.extend(float(\"NaN\") for _ in range(14))\n",
    "    temp.append(total_const)  \n",
    "    row.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read column names from output file\n",
    "column_df=pd.read_excel(\"Output Data Structure.xlsx\",header=None)\n",
    "column=list(column_df.iloc[0,6:])\n",
    "\n",
    "#merge two dataframes \n",
    "df2=pd.DataFrame(row,columns=column)\n",
    "result=pd.concat([df1, df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert csv file to xlsx\n",
    "writer=pd.ExcelWriter('output.xlsx')\n",
    "result.to_excel(writer, index = False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "rel_url=\"edgar/data/4962/0001193125-14-167067.txt\"\n",
    "url=parse.urljoin(abs_url,rel_url)\n",
    "text=requests.get(url)\n",
    "    \n",
    "#parse html using beautiful soup\n",
    "soup=BeautifulSoup(text.content,\"lxml\")\n",
    "for tag in soup([\"sec-header\"]):\n",
    "    tag.decompose()\n",
    "text=soup.get_text()\n",
    "cleaned_text=clean_text(text)\n",
    "print(text)\n",
    "#print(cleaned_text)\n",
    "sections=[\" MANAGEMENTS DISCUSSION AND ANALYSIS\", \" QUANTITATIVE AND QUALITATIVE DISCLOSUR(?:ES|E) ABOUT MARKET RISK\", \" RISK FACTORS\"]\n",
    "pattern=r\"\\. (?:ITEM|Item) [A-Za-z]?\\.\"+sections[0]+r\"(.+?)(?:ITEM|Item) [A-Za-z]?\\.\"\n",
    "match=re.compile(pattern,flags=re.DOTALL)\n",
    "substring=match.search(cleaned_text)\n",
    "substring=substring.group(1)\n",
    "#print(substring)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
